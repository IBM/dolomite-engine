# **************************************************
# Copyright (c) 2025, Mayank Mishra
# **************************************************

datasets:
  # class_name, data_name & data_sampling_ratio are not used but need to be passed to avoid errors
  - class_name: FSDPDataset
    data_name: FSDP
    data_sampling_ratio: 1
    class_args:
      enable_checkpoint: True
      checkpoint_model_weights_only: False
      checkpoint_folder: checkpoints
      dataset_path: /proj/datasets/fms-dataloader-test/
      datasets: null #"dump=CC-MAIN-2013-20,dump=CC-MAIN-2015-35,dump=CC-MAIN-2017-26,dump=CC-MAIN-2018-47,dump=CC-MAIN-2020-24,dump=CC-MAIN-2022-27"
      weights: null #"1,1,1,1,1,1"
      # dataset_path: "/proj/data-eng/fsdp/data/R45_th7_200"
      # datasets: "Java,Python"
      # weights: "0.5,0.5"
      col_name: "tokens"
      file_type: "arrow"
      vocab_size: 200000
      bos_token: null
      eos_token: 0
      drop_tokens: ""
      logical_shards: 8 # 8GPUs * 2 Nodes * 4 workers * 2 (Some multiple)
      num_workers: 1
      seed: 2023
      sequence_length: 2048 # This sets the seq_len
 
tokenizer_args:
  tokenizer_name: bigcode/starcoder

model_args:
  model_class: AutoModelForCausalLM
  pretrained_config:
    model_type: gpt_dolomite
    vocab_size: 50257
    max_position_embeddings: 2048
    hidden_size: 768
    num_layers: 12
    normalization_function: layernorm
    layer_norm_epsilon: 1e-5
    initializer_range: 0.02
    bos_token_id: 0
    eos_token_id: 0
    pad_token_id: 0
    sequence_mixer_blocks:
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        attention_head_type: mqa
        num_key_value_heads: 1
    mlp_blocks:
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
      - mlp_type: MLP
        activation_function: gelu_pytorch_tanh
        intermediate_size: 3072
        add_bias: true
    position_embedding_type: learned_absolute
  use_padding_free_transformer: true

tuning_args:
  tuning_method: pretraining

save_args:
  save_path: checkpoints
  save_interval: 50

training_parameters:
  num_training_steps: 100
  eval_interval: 50
  micro_batch_size: 6

optimizer_args:
  class_name: TorchAdamW
  class_args:
    lr: 1e-5
    weight_decay: 0.1
    betas:
      - 0.9
      - 0.95
    eps: 1e-10

lr_scheduler_args:
  lr_decay_style: cosine

mixed_precision_args:
  dtype: bf16
