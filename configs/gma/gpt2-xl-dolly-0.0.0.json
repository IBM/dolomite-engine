{
    "datasets": [
        {
            "data_class": "DollyDataset",
            "data_name": "dolly",
            "data_path": "../dolly/data/",
            "data_sampling_proportion": 1,
            "output_format": " __output__",
            "max_input_tokens": 750,
            "max_output_tokens": 274
        }
    ],
    "model_name": "gpt2-xl",
    "model_class": "AutoModelForCausalLM",
    "training_inference_type": "full_finetuning",
    "logdir": "/cos/mayank/aim-repo",
    "experiment_name": "gma-gpt2-xl-dolly-0.0.0",
    "save_path": "/cos/mayank/checkpoints/gma-gpt2-xl-dolly-0.0.0",
    "num_training_steps": 5000,
    "eval_and_save_interval": 1000,
    "batch_size_per_gpu": 4,
    "dtype": "bfloat16",
    "optimizer": {
        "optimizer_class": "ApexFusedAdam",
        "lr": 1e-4,
        "weight_decay": 0.1,
        "betas": [0.9, 0.95],
        "eps": 1e-10
    },
    "lr_schedule": "cosine"
}
