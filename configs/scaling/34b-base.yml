datasets:
  # class_name, data_name & data_sampling_ratio are not used but need to be passed to avoid errors
  - class_name: MegatronDataset
    data_name: Megatron
    data_sampling_ratio: 1
    class_args:
      eval_steps: 2
      # data_cache_path: cache
      # Option 1: data loading using --data-path with single file
      data_path:
        - 1
        - /proj/datasets/slim_pajama_gptneox_megatron/train/chunk1
        # - 58982360
        # - /proj/datasets/slim_pajama_gptneox_megatron/train/chunk2
        # - 59060327
        # - /proj/datasets/slim_pajama_gptneox_megatron/train/chunk3
        # - 59040311
        # - /proj/datasets/slim_pajama_gptneox_megatron/train/chunk4
        # - 59201586
        # - /proj/datasets/slim_pajama_gptneox_megatron/train/chunk5
        # - 59023939
        # - /proj/datasets/slim_pajama_gptneox_megatron/train/chunk6
        # - 58932495
        # - /proj/datasets/slim_pajama_gptneox_megatron/train/chunk7
        # - 59087730
        # - /proj/datasets/slim_pajama_gptneox_megatron/train/chunk8
        # - 59073554
        # - /proj/datasets/slim_pajama_gptneox_megatron/train/chunk9
        # - 58995987
        # - /proj/datasets/slim_pajama_gptneox_megatron/train/chunk10
      split: 98,1,1
      sequence_length: 8192

tokenizer_args:
  tokenizer_name: EleutherAI/gpt-neox-20b

model_args:
  model_class: AutoModelForCausalLM
  pretrained_config:
    activation_function: gelu_pytorch_tanh
    add_bias: false
    attention_softmax_in_fp32: true
    attn_pdrop: 0
    embd_pdrop: 0
    initializer_range: 0.02
    layer_norm_epsilon: 1e-05
    model_type: gpt_megatron
    n_embd: 6144
    n_head: 48
    n_layer: 88
    attention_head_type: mqa
    n_positions: 8192
    normalization_function: layernorm
    resid_pdrop: 0
    scale_attention_softmax_in_fp32: true
    scale_attn_weights: true
    vocab_size: 50304
  efficient_initialization: true
  attention_implementation: sdpa
  # use_padding_free_transformer: true

tuning_args:
  tuning_method: pretraining

save_args:
  save_path: /proj/checkpoints/mayank/mla-experiments/8b
  save_interval: 5000

training_parameters:
  num_training_steps: 2500
  eval_interval: 2500000
  micro_batch_size: 1
  gradient_accumulation_steps: 2

optimizer_args:
  class_name: TorchAdamW
  class_args:
    lr: 3e-4
    weight_decay: 0.1
    betas:
      - 0.9
      - 0.95
    eps: 1e-10

lr_scheduler_args:
  lr_decay_style: cosine
  num_warmup_steps: 200
  num_constant_steps: 2230

mixed_precision_args:
  dtype: bf16

distributed_args:
  distributed_backend: torch
  stage: 3
  # zero_hpz_partition_size: 8
  # gradient_checkpointing: true
